#!/usr/bin/env python3
"""
NETGEAR 2025å¹´å¢žå¼ºæ•°æ®çˆ¬å–è„šæœ¬
- èŽ·å–Q1/Q2 2025å¹´Alpha Vantageè´¢åŠ¡æ•°æ®
- æœç´¢SECä¸šåŠ¡åˆ†æ®µæ•°æ®
- æ¸…ç†å¼‚å¸¸æ•°æ®
- è¡¥å……æŠ•èµ„è€…å…³ç³»é¡µé¢ä¿¡æ¯
"""

import os
import requests
import json
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any
from supabase import create_client
from dotenv import load_dotenv
import time

# åŠ è½½çŽ¯å¢ƒå˜é‡
load_dotenv()

class Enhanced2025DataCrawler:
    def __init__(self):
        self.setup_logging()
        self.setup_clients()
        self.netgear_company_id = None
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_filename = f'enhanced_2025_crawler_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_filename),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def setup_clients(self):
        """åˆå§‹åŒ–å®¢æˆ·ç«¯"""
        # Supabaseå®¢æˆ·ç«¯
        supabase_url = os.getenv('SUPABASE_URL') or os.getenv('NEXT_PUBLIC_SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_ANON_KEY') or os.getenv('NEXT_PUBLIC_SUPABASE_ANON_KEY')
        
        if not supabase_url or not supabase_key:
            raise ValueError("Supabaseå‡­æ®æœªæ‰¾åˆ°")
            
        self.supabase = create_client(supabase_url, supabase_key)
        
        # Alpha Vantage APIå¯†é’¥
        self.alpha_vantage_key = os.getenv('ALPHA_VANTAGE_API_KEY')
        if not self.alpha_vantage_key:
            raise ValueError("Alpha Vantage APIå¯†é’¥æœªæ‰¾åˆ°")
            
        self.logger.info("âœ… å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        
    def get_company_id(self) -> str:
        """èŽ·å–NETGEARå…¬å¸ID"""
        if self.netgear_company_id:
            return self.netgear_company_id
            
        result = self.supabase.table('companies').select('id').eq('symbol', 'NTGR').execute()
        if not result.data:
            raise ValueError("æœªæ‰¾åˆ°NETGEARå…¬å¸è®°å½•")
            
        self.netgear_company_id = result.data[0]['id']
        return self.netgear_company_id
        
    def clean_abnormal_2025_data(self):
        """æ¸…ç†å¼‚å¸¸çš„2025å¹´è´¢åŠ¡æ•°æ®"""
        self.logger.info("ðŸ§¹ å¼€å§‹æ¸…ç†å¼‚å¸¸çš„2025å¹´è´¢åŠ¡æ•°æ®...")
        
        company_id = self.get_company_id()
        
        # æŸ¥æ‰¾2025å¹´çš„å¼‚å¸¸æ•°æ®ï¼ˆè¥æ”¶è¶…è¿‡1000äº¿ç¾Žå…ƒçš„æ˜Žæ˜¾é”™è¯¯æ•°æ®ï¼‰
        result = self.supabase.table('financial_data').select('*').eq('company_id', company_id).eq('fiscal_year', 2025).execute()
        
        abnormal_records = []
        for record in result.data:
            if record['revenue'] and record['revenue'] > 100000000000:  # è¶…è¿‡1000äº¿ç¾Žå…ƒ
                abnormal_records.append(record)
                
        if abnormal_records:
            self.logger.warning(f"å‘çŽ° {len(abnormal_records)} æ¡å¼‚å¸¸çš„2025å¹´æ•°æ®")
            for record in abnormal_records:
                revenue_b = record['revenue'] / 1000000000
                self.logger.warning(f"  - Q{record['fiscal_quarter']}-2025: ${revenue_b:.1f}B (å¼‚å¸¸)")
                
                # åˆ é™¤å¼‚å¸¸è®°å½•
                self.supabase.table('financial_data').delete().eq('id', record['id']).execute()
                self.logger.info(f"  âœ… å·²åˆ é™¤å¼‚å¸¸è®°å½•: Q{record['fiscal_quarter']}-2025")
        else:
            self.logger.info("âœ… æœªå‘çŽ°å¼‚å¸¸çš„2025å¹´è´¢åŠ¡æ•°æ®")
            
    def fetch_alpha_vantage_2025_data(self):
        """èŽ·å–2025å¹´Alpha Vantageè´¢åŠ¡æ•°æ®"""
        self.logger.info("ðŸ“Š å¼€å§‹èŽ·å–2025å¹´Alpha Vantageè´¢åŠ¡æ•°æ®...")
        
        company_id = self.get_company_id()
        
        # èŽ·å–å­£åº¦è´¢åŠ¡æ•°æ®
        url = f"https://www.alphavantage.co/query"
        params = {
            'function': 'INCOME_STATEMENT',
            'symbol': 'NTGR',
            'apikey': self.alpha_vantage_key
        }
        
        try:
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            if 'quarterlyReports' not in data:
                self.logger.error(f"Alpha Vantageå“åº”å¼‚å¸¸: {data}")
                return False
                
            quarterly_reports = data['quarterlyReports']
            self.logger.info(f"èŽ·å–åˆ° {len(quarterly_reports)} ä¸ªå­£åº¦æŠ¥å‘Š")
            
            # å¤„ç†2025å¹´æ•°æ®
            inserted_count = 0
            for report in quarterly_reports:
                fiscal_date = report.get('fiscalDateEnding', '')
                if not fiscal_date.startswith('2025'):
                    continue
                    
                # è§£æžå­£åº¦
                quarter = self.parse_quarter_from_date(fiscal_date)
                if not quarter:
                    continue
                    
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = self.supabase.table('financial_data').select('id').eq('company_id', company_id).eq('fiscal_year', 2025).eq('fiscal_quarter', quarter).eq('data_source', 'alpha_vantage').execute()
                
                if existing.data:
                    self.logger.info(f"Q{quarter}-2025 Alpha Vantageæ•°æ®å·²å­˜åœ¨")
                    continue
                
                # è§£æžè´¢åŠ¡æ•°æ®
                revenue = self.parse_financial_value(report.get('totalRevenue'))
                gross_profit = self.parse_financial_value(report.get('grossProfit'))
                net_income = self.parse_financial_value(report.get('netIncome'))
                operating_expenses = self.parse_financial_value(report.get('operatingExpenses'))
                
                if not revenue:
                    self.logger.warning(f"Q{quarter}-2025: è¥æ”¶æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
                    continue
                
                # æ’å…¥æ•°æ®
                financial_record = {
                    'company_id': company_id,
                    'period': f'Q{quarter}-2025',
                    'fiscal_year': 2025,
                    'fiscal_quarter': quarter,
                    'revenue': revenue,
                    'gross_profit': gross_profit,
                    'net_income': net_income,
                    'operating_expenses': operating_expenses,
                    'data_source': 'alpha_vantage',
                    'source_date': fiscal_date
                }
                
                result = self.supabase.table('financial_data').insert(financial_record).execute()
                if result.data:
                    inserted_count += 1
                    revenue_m = revenue / 1000000 if revenue else 0
                    self.logger.info(f"âœ… æ’å…¥Q{quarter}-2025è´¢åŠ¡æ•°æ®: ${revenue_m:.1f}M")
                    
            self.logger.info(f"âœ… æˆåŠŸæ’å…¥ {inserted_count} æ¡2025å¹´Alpha Vantageè´¢åŠ¡æ•°æ®")
            return True
            
        except Exception as e:
            self.logger.error(f"èŽ·å–Alpha Vantageæ•°æ®å¤±è´¥: {e}")
            return False
            
    def search_sec_2025_segments(self):
        """æœç´¢2025å¹´SECä¸šåŠ¡åˆ†æ®µæ•°æ®"""
        self.logger.info("ðŸ” å¼€å§‹æœç´¢2025å¹´SECä¸šåŠ¡åˆ†æ®µæ•°æ®...")
        
        # æœç´¢NETGEAR SEC EDGARæ–‡ä»¶
        search_urls = [
            "https://www.sec.gov/edgar/search/#/q=NETGEAR&dateRange=custom&startdt=2025-01-01&enddt=2025-07-31",
            "https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0001122904&type=10-Q&dateb=&owner=exclude&count=10"
        ]
        
        # ç”±äºŽSEC EDGAR APIé™åˆ¶ï¼Œè¿™é‡Œè®°å½•æœç´¢å»ºè®®
        self.logger.info("ðŸ”— å»ºè®®æ‰‹åŠ¨æ£€æŸ¥ä»¥ä¸‹SEC EDGARé“¾æŽ¥:")
        for url in search_urls:
            self.logger.info(f"   - {url}")
            
        # å°è¯•ä»Žå·²çŸ¥çš„NETGEARå­£åº¦æŠ¥å‘ŠURLæ¨¡å¼èŽ·å–
        potential_urls = [
            "https://investor.netgear.com/sec-filings",
            "https://www.sec.gov/Archives/edgar/data/1122904/"
        ]
        
        self.logger.info("ðŸ’¡ å»ºè®®çš„SECæ•°æ®èŽ·å–ç­–ç•¥:")
        self.logger.info("   1. è®¿é—® investor.netgear.com/sec-filings")
        self.logger.info("   2. ä¸‹è½½æœ€æ–°çš„10-QæŠ¥å‘Š (Q1 2025, Q2 2025)")
        self.logger.info("   3. æœç´¢ 'revenue by segment' æˆ– 'Connected Home' å…³é”®è¯")
        self.logger.info("   4. æ‰‹åŠ¨æå–ä¸šåŠ¡åˆ†æ®µæ•°æ®")
        
        return False  # éœ€è¦æ‰‹åŠ¨å¤„ç†
        
    def fetch_investor_relations_data(self):
        """èŽ·å–æŠ•èµ„è€…å…³ç³»é¡µé¢æ•°æ®"""
        self.logger.info("ðŸ‘” å¼€å§‹æœç´¢æŠ•èµ„è€…å…³ç³»é¡µé¢æ•°æ®...")
        
        # NETGEARæŠ•èµ„è€…å…³ç³»é¡µé¢URL
        urls_to_check = [
            "https://investor.netgear.com/releases/press-release-details/2025/NETGEAR-Reports-First-Quarter-2025-Results/default.aspx",
            "https://investor.netgear.com/releases/press-release-details/2025/NETGEAR-Reports-Second-Quarter-2025-Results/default.aspx",
            "https://investor.netgear.com/financials/quarterly-results/default.aspx"
        ]
        
        self.logger.info("ðŸ”— å‘çŽ°ä»¥ä¸‹æŠ•èµ„è€…å…³ç³»æ•°æ®æº:")
        for url in urls_to_check:
            self.logger.info(f"   - {url}")
            
        # è®°å½•éœ€è¦æå–çš„å…³é”®æ•°æ®ç‚¹
        key_data_points = [
            "Connected Home segment revenue",
            "NETGEAR for Business segment revenue", 
            "Product line breakdown",
            "Geographic revenue distribution",
            "Gross margins by segment"
        ]
        
        self.logger.info("ðŸ“‹ å»ºè®®æå–çš„å…³é”®æ•°æ®ç‚¹:")
        for point in key_data_points:
            self.logger.info(f"   - {point}")
            
        return True
        
    def parse_quarter_from_date(self, date_str: str) -> Optional[int]:
        """ä»Žæ—¥æœŸå­—ç¬¦ä¸²è§£æžå­£åº¦"""
        try:
            from datetime import datetime
            date_obj = datetime.strptime(date_str, '%Y-%m-%d')
            month = date_obj.month
            
            if month in [1, 2, 3]:
                return 1
            elif month in [4, 5, 6]:
                return 2
            elif month in [7, 8, 9]:
                return 3
            elif month in [10, 11, 12]:
                return 4
        except:
            pass
        return None
        
    def parse_financial_value(self, value_str: str) -> Optional[int]:
        """è§£æžè´¢åŠ¡æ•°å€¼"""
        if not value_str or value_str == 'None':
            return None
        try:
            return int(float(value_str))
        except:
            return None
            
    def log_update(self, operation: str, records_count: int, status: str):
        """è®°å½•æ•°æ®åº“æ›´æ–°æ—¥å¿—"""
        try:
            log_record = {
                'operation_type': operation,
                'records_affected': records_count,
                'status': status,
                'details': f'Enhanced 2025 data crawler - {operation}',
                'script_name': 'enhanced_2025_data_crawler.py'
            }
            self.supabase.table('data_update_log').insert(log_record).execute()
        except Exception as e:
            self.logger.warning(f"è®°å½•æ›´æ–°æ—¥å¿—å¤±è´¥: {e}")
            
    def run_full_crawl(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®èŽ·å–æµç¨‹"""
        self.logger.info("ðŸš€ å¯åŠ¨å¢žå¼ºçš„2025å¹´æ•°æ®èŽ·å–æµç¨‹")
        self.logger.info("=" * 60)
        
        success_count = 0
        total_operations = 4
        
        try:
            # 1. æ¸…ç†å¼‚å¸¸æ•°æ®
            self.logger.info("æ­¥éª¤ 1/4: æ¸…ç†å¼‚å¸¸æ•°æ®")
            self.clean_abnormal_2025_data()
            success_count += 1
            
            # 2. èŽ·å–Alpha Vantage 2025æ•°æ®
            self.logger.info("æ­¥éª¤ 2/4: èŽ·å–Alpha Vantage 2025æ•°æ®")
            if self.fetch_alpha_vantage_2025_data():
                success_count += 1
                self.log_update('alpha_vantage_2025_fetch', 2, 'success')
            else:
                self.log_update('alpha_vantage_2025_fetch', 0, 'failed')
            
            # 3. æœç´¢SEC 2025ä¸šåŠ¡åˆ†æ®µæ•°æ®
            self.logger.info("æ­¥éª¤ 3/4: æœç´¢SECä¸šåŠ¡åˆ†æ®µæ•°æ®")
            self.search_sec_2025_segments()
            success_count += 1  # ä¿¡æ¯æ”¶é›†æˆåŠŸ
            
            # 4. èŽ·å–æŠ•èµ„è€…å…³ç³»æ•°æ®
            self.logger.info("æ­¥éª¤ 4/4: æ”¶é›†æŠ•èµ„è€…å…³ç³»æ•°æ®æº")
            if self.fetch_investor_relations_data():
                success_count += 1
                
        except Exception as e:
            self.logger.error(f"æ•°æ®èŽ·å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            self.log_update('enhanced_2025_crawl', 0, 'failed')
            
        self.logger.info("=" * 60)
        self.logger.info(f"ðŸŽ¯ æ•°æ®èŽ·å–æµç¨‹å®Œæˆ: {success_count}/{total_operations} æ­¥éª¤æˆåŠŸ")
        
        if success_count >= 2:
            self.logger.info("âœ… æ ¸å¿ƒæ•°æ®èŽ·å–æˆåŠŸå®Œæˆ!")
        else:
            self.logger.warning("âš ï¸ éƒ¨åˆ†æ­¥éª¤éœ€è¦æ‰‹åŠ¨å¤„ç†")
            
        return success_count >= 2

def main():
    """ä¸»å‡½æ•°"""
    try:
        crawler = Enhanced2025DataCrawler()
        success = crawler.run_full_crawl()
        exit(0 if success else 1)
    except Exception as e:
        logging.error(f"è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        exit(1)

if __name__ == "__main__":
    main()