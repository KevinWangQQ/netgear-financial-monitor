name: Update Database with Latest Financial Data

on:
  # ÊØèÂ§©UTCÊó∂Èó¥ÂáåÊô®2ÁÇπËøêË°åÔºàÂØπÂ∫îÂåó‰∫¨Êó∂Èó¥‰∏äÂçà10ÁÇπÔºâ
  schedule:
    - cron: '0 2 * * *'
  
  # ÂÖÅËÆ∏ÊâãÂä®Ëß¶Âèë
  workflow_dispatch:
    inputs:
      force_update:
        description: 'Force update all data'
        required: false
        default: 'false'
        type: boolean

jobs:
  update-financial-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install Python dependencies
      run: |
        cd scripts
        python -m pip install --upgrade pip
        pip install requests supabase python-dotenv
        
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'npm'
        
    - name: Install Node.js dependencies
      run: npm ci
      
    - name: Run financial data crawler
      env:
        ALPHA_VANTAGE_API_KEY: ${{ secrets.ALPHA_VANTAGE_API_KEY }}
        NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
        NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
        FORCE_UPDATE: ${{ github.event.inputs.force_update }}
      run: |
        cd scripts
        echo "ÂºÄÂßãÊõ¥Êñ∞Ë¥¢Âä°Êï∞ÊçÆ..."
        python run_crawler_simple.py
        
    - name: Test database connection
      env:
        ALPHA_VANTAGE_API_KEY: ${{ secrets.ALPHA_VANTAGE_API_KEY }}
        NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
        NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
      run: |
        cd scripts
        echo "ÊµãËØïÊï∞ÊçÆÂ∫ìËøûÊé•..."
        python test_connection.py
        
    - name: Verify database tables
      env:
        NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
        NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
      run: |
        cd scripts
        echo "È™åËØÅÊï∞ÊçÆÂ∫ìË°®ÁªìÊûÑ..."
        python check_tables.py
        
    - name: Test application build
      run: |
        echo "ÊµãËØïÂ∫îÁî®ÊûÑÂª∫..."
        npm run build
        
    - name: Deploy to production (if on main branch)
      if: github.ref == 'refs/heads/main' && github.event_name == 'schedule'
      env:
        VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
        VERCEL_ORG_ID: ${{ secrets.VERCEL_ORG_ID }}
        VERCEL_PROJECT_ID: ${{ secrets.VERCEL_PROJECT_ID }}
      run: |
        echo "Ëß¶ÂèëÁîü‰∫ßÈÉ®ÁΩ≤..."
        npx vercel --prod --token $VERCEL_TOKEN
        
    - name: Create summary report
      if: always()
      run: |
        echo "## Êï∞ÊçÆÂ∫ìÊõ¥Êñ∞Êä•Âëä - $(date)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ÊâßË°åÁä∂ÊÄÅ" >> $GITHUB_STEP_SUMMARY
        
        if [ -f "scripts/crawler_log.log" ]; then
          echo "‚úÖ Êï∞ÊçÆÊäìÂèñÂÆåÊàê" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ÊäìÂèñÊó•Âøó" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          tail -20 scripts/crawler_log.log >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
        else
          echo "‚ö†Ô∏è Êï∞ÊçÆÊäìÂèñÊó•ÂøóÊú™ÊâæÂà∞" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ‰∏ãÊ¨°ÊâßË°åÊó∂Èó¥" >> $GITHUB_STEP_SUMMARY
        echo "$(date -d '+1 day' '+%Y-%m-%d 02:00 UTC')" >> $GITHUB_STEP_SUMMARY
        
  # Êï∞ÊçÆË¥®ÈáèÊ£ÄÊü•‰ªªÂä°
  data-quality-check:
    needs: update-financial-data
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        cd scripts
        pip install requests supabase python-dotenv
        
    - name: Run data quality checks
      env:
        NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
        NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
      run: |
        cd scripts
        python << 'EOF'
        import os
        from supabase import create_client
        from dotenv import load_dotenv
        
        # ÁÆÄÂçïÁöÑÊï∞ÊçÆË¥®ÈáèÊ£ÄÊü•
        supabase_url = os.getenv('NEXT_PUBLIC_SUPABASE_URL')
        supabase_key = os.getenv('NEXT_PUBLIC_SUPABASE_ANON_KEY')
        supabase = create_client(supabase_url, supabase_key)
        
        print("üîç Êï∞ÊçÆË¥®ÈáèÊä•Âëä")
        print("=" * 40)
        
        # Ê£ÄÊü•Ë¥¢Âä°Êï∞ÊçÆ
        financial_data = supabase.table('financial_data').select('*').limit(5).execute()
        print(f"üìä Ë¥¢Âä°Êï∞ÊçÆ: {len(financial_data.data)} Êù°ÊúÄÊñ∞ËÆ∞ÂΩï")
        
        # Ê£ÄÊü•‰∫ßÂìÅÁ∫øÊï∞ÊçÆ
        product_data = supabase.table('product_line_revenue').select('*').limit(5).execute()
        print(f"üì¶ ‰∫ßÂìÅÁ∫øÊï∞ÊçÆ: {len(product_data.data)} Êù°ËÆ∞ÂΩï")
        
        # Ê£ÄÊü•Âú∞ÁêÜÂàÜÂ∏ÉÊï∞ÊçÆ
        geo_data = supabase.table('geographic_revenue').select('*').limit(5).execute()
        print(f"üåç Âú∞ÁêÜÂàÜÂ∏ÉÊï∞ÊçÆ: {len(geo_data.data)} Êù°ËÆ∞ÂΩï")
        
        # Ê£ÄÊü•ÈáåÁ®ãÁ¢ë‰∫ã‰ª∂
        events = supabase.table('milestone_events').select('*').limit(5).execute()
        print(f"üéØ ÈáåÁ®ãÁ¢ë‰∫ã‰ª∂: {len(events.data)} Êù°ËÆ∞ÂΩï")
        
        print("\n‚úÖ Êï∞ÊçÆË¥®ÈáèÊ£ÄÊü•ÂÆåÊàê")
        EOF